{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THIS CELL ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current path of the notebook\n",
    "\n",
    "notebook_path = os.getcwd()\n",
    "# Construct the path to the directory containing 'fit'\n",
    "parent_path = os.path.join(notebook_path, '..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ckpts/t_tf.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "os.chdir(parent_path)\n",
    "\n",
    "import sys\n",
    "# Add this parent directory to the system path\n",
    "sys.path.insert(0, parent_path)\n",
    "from fit.fit_pnf import PNF as PNFModel\n",
    "from fit.fit_nf import NF as NFModel\n",
    "from fit.fit_tnf import TNF as TNFModel\n",
    "from fit.fit_tf import TF as TFModel\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from utils.dataloader_jetnet import PointCloudDataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from utils.helpers import get_hists, mass\n",
    "from utils.dataloader_jetnet import PointCloudDataloader\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "from jetnet.evaluation import w1m\n",
    "from utils.helpers import mass, plotting_thesis,fit_kde,sample_kde,create_mask\n",
    "\n",
    "\n",
    "def calculate_data_bounds(dataloader):\n",
    "    \"\"\"\n",
    "    Calculates the minimum and maximum values across the dimensions from the dataloader.\n",
    "\n",
    "    :param dataloader: The dataloader to process.\n",
    "    :param n_dim: Number of dimensions.\n",
    "    :return: Tuple of minimum values, maximum values, and count of non-masked data points.\n",
    "    \"\"\"\n",
    "    mins = torch.ones(3).unsqueeze(0)\n",
    "    maxs = torch.ones(3).unsqueeze(0)\n",
    "    n = []\n",
    "    for i in dataloader:\n",
    "        mins = torch.min(\n",
    "            torch.cat((mins, i[0][~i[1]].min(0, keepdim=True)[0]), dim=0), dim=0\n",
    "        )[0].unsqueeze(0)\n",
    "        maxs = torch.max(\n",
    "            torch.cat((maxs, i[0][~i[1]].max(0, keepdim=True)[0]), dim=0), dim=0\n",
    "        )[0].unsqueeze(0)\n",
    "        n.append((~i[1]).sum(1))\n",
    "    # model.maxs = maxs.cuda()\n",
    "    # model.mins = mins.cuda()\n",
    "    # model.avg_n = torch.cat(n, dim=0).float().cuda().mean()\n",
    "    return mins, maxs, n\n",
    "\n",
    "def setup_model_with_data(model, data_module):\n",
    "    \"\"\"\n",
    "    Sets up the model with the data module and configuration parameters.\n",
    "\n",
    "    :param model: The model to be set up.\n",
    "    :param data_module: The data module used for training and validation.\n",
    "    :param config: Configuration dictionary.\n",
    "    \"\"\"\n",
    "    model.bins = [100, 100, 100, 100]\n",
    "    model.n_dim = 3\n",
    "    model.scaler = data_module.scaler[0]\n",
    "    model.pt_scaler = data_module.scaler[1]\n",
    "    model.w1m_best = 0.01\n",
    "\n",
    "\n",
    "    # Calculate the minimum and maximum values from the training data\n",
    "    mins, maxs, n_counts = calculate_data_bounds(data_module.train_dataloader())\n",
    "    model.maxs = maxs.cuda()\n",
    "    model.mins = mins.cuda()\n",
    "    model.avg_n = torch.cat(n_counts, dim=0).float().cuda().mean()\n",
    "    model.gen_net.avg_n = torch.cat(n_counts, dim=0).float().cuda().mean()\n",
    "    model.dis_net.avg_n = torch.cat(n_counts, dim=0).float().cuda().mean()\n",
    "    # Additional model settings\n",
    "    model.scaler = model.scaler.to(\"cuda\")\n",
    "    model.scaler.std = model.scaler.std.cuda()\n",
    "    model.scaled_mins = torch.tensor(data_module.mins).cuda()\n",
    "    model.scaled_maxs = torch.tensor(data_module.maxs).cuda()\n",
    "\n",
    "\n",
    "\n",
    "def make_plots(model_name, disco=False):\n",
    "\n",
    "    ckptdir = \"./ckpts/\"\n",
    "    ckpt = \"t_{}.ckpt\".format(model_name)\n",
    "    # ckpt = \"t_{}.ckpt\".format(model_name)\n",
    "    ckpt = ckptdir + ckpt\n",
    "    print(ckpt)\n",
    "\n",
    "    # Load state dictionary from checkpoint\n",
    "    # state_dict = torch.load(ckpt)\n",
    "    # config = state_dict[\"hyper_parameters\"]\n",
    "    # config[\"model_name\"] = model_name\n",
    "\n",
    "    # print(config)\n",
    "    # Choose the model class based on the model name\n",
    "    if model_name in [\"ipf\", \"pf\", \"apf\"]:\n",
    "        # config[\"pf\"] = True\n",
    "        # config[\"adversarial\"] = False\n",
    "        # config[\"norm\"]=False\n",
    "        # config[\"fast\"]=False\n",
    "\n",
    "        model_class = PNFModel\n",
    "    else:\n",
    "        # config[\"pf\"] = False\n",
    "        model_class = NFModel if model_name.find(\"t\")==-1 else TNFModel if model_name.find(\"tnf\")>-1 else TFModel\n",
    "\n",
    "    torch.set_float32_matmul_precision('medium' )\n",
    "    model = model_class.load_from_checkpoint(ckpt,ema=False)\n",
    "\n",
    "    # Initialize data module and set up model\n",
    "    data_module = PointCloudDataloader(parton=\"t\",n_dim=3,n_part=30,batch_size=1024,sampler=False)\n",
    "    data_module.setup(\"fit\")\n",
    "\n",
    "\n",
    "    # Assuming `model` is defined elsewhere in your code\n",
    "    setup_model_with_data(model, data_module)\n",
    "    train=data_module.train_dataloader().dataset.cuda()\n",
    "    test=data_module.test_dataloader().dataset.cuda()\n",
    "    pt=model.pt_scaler.inverse_transform(train[:,:,-2])\n",
    "    std=model.scaler.inverse_transform(train[:,:,:-2])\n",
    "    train=torch.cat((std,pt.unsqueeze(2),train[:,:,-1:]),dim=2)\n",
    "    data=torch.cat((train,test[:,:,:]),dim=0)\n",
    "    m=mass(data.cuda()).cpu()\n",
    "    n=(~(torch.cat((train,test),dim=0)[:,:,-1]).bool()).float().sum(1).cpu()\n",
    "    n_kde,m_kde=fit_kde(n,m)\n",
    "    n,m=sample_kde(len(data),n_kde,m_kde)\n",
    "    # Trainer setup and model validation\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"gpu\")\n",
    "    model.eval_metrics=False\n",
    "    model.batch=[]\n",
    "    model.masks=[]\n",
    "    model.fake=[]\n",
    "    model.conds=[]\n",
    "    model=model.cuda()\n",
    "    model.load_datamodule(data_module)\n",
    "    with torch.no_grad():\n",
    "        trainer.test(model, data_module.val_dataloader())\n",
    "\n",
    "    # concatenate all batches\n",
    "    fake = torch.cat(model.fake)\n",
    "    true = torch.cat(model.batch)\n",
    "    # sorted_indices = torch.argsort(fake[:,:,2], dim=1, descending=True)\n",
    "    # fake = torch.gather(fake, 1, sorted_indices.unsqueeze(-1).expand(-1, -1, fake.shape[2]))\n",
    "\n",
    "    m_f, m_t = mass(fake), mass(true)\n",
    "\n",
    "    # Apply clamping based on quantiles\n",
    "    mins = torch.quantile(true.reshape(-1, 3), 0.001, dim=0)\n",
    "    maxs = torch.quantile(true.reshape(-1, 3), 0.999, dim=0)\n",
    "    fake = torch.clamp(fake, min=mins, max=maxs)\n",
    "    true = torch.clamp(true, min=mins, max=maxs)\n",
    "    m_f = torch.clamp(m_f, min=torch.quantile(m_t, 0.001), max=torch.quantile(m_t, 0.999))\n",
    "    m_t = torch.clamp(m_t, min=torch.quantile(m_t, 0.001), max=torch.quantile(m_t, 0.999))\n",
    "    mins=torch.cat([mins,torch.tensor([torch.quantile(m_t, 0.001)])])-0.01\n",
    "    maxs=torch.cat([maxs,torch.tensor([torch.quantile(m_t, 0.999)])])*1.01\n",
    "    for i in range(5):\n",
    "        w1m_=w1m(fake,true,num_batches=16,num_eval_samples=250000)\n",
    "        print(w1m_)\n",
    "    # Prepare histograms\n",
    "    hists=get_hists([30,30,30,30],mins*1.1,maxs*1.1,calo=model.name==\"calo\")\n",
    "\n",
    "    masks=torch.cat(model.masks)\n",
    "    # Fill histograms\n",
    "    for var in range(3):\n",
    "        hists[\"hists_real\"][var].fill(true.reshape(-1, 3)[(true.reshape(-1, 3) != 0).all(1)][:, var].cpu().numpy())\n",
    "        hists[\"hists_fake\"][var].fill(fake.reshape(-1, 3)[(fake.reshape(-1, 3) != 0).all(1)][:, var].cpu().numpy())\n",
    "\n",
    "\n",
    "    hists[\"hists_real\"][3].fill(m_t.cpu().numpy())\n",
    "    hists[\"hists_fake\"][3].fill(m_f.cpu().numpy())\n",
    "\n",
    "    # Plotting\n",
    "    plotter = plotting_thesis()\n",
    "    plotter.plot_ratio(hists[\"hists_real\"], hists[\"hists_fake\"], weighted=False, leg=2, model_name=model_name)\n",
    "    # plotter.plot_corr(true.numpy(), fake.numpy(), model_name, disco=disco,leg=-1)\n",
    "    return model\n",
    "pointflows=[ \"ipf\", \"apf\", \"pf\",]\n",
    "flows=[\"nf\",\"cnf\",\"ccnf\"]\n",
    "gans=[\"tnf\",\"tnf_fast\",\"tf_slow\",\"tf_slow_ema\"]\n",
    "\n",
    "for model_name in  [\"tf\"]:#\n",
    "    model=make_plots(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 30, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(model.fake).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "ckpt=torch.load(\"/beegfs/desy/user/kaechben/thesis/eval_jetnet150/generated_data_50000.pt\")\n",
    "with open(\"/beegfs/desy/user/kaechben/thesis/eval_jetnet150/EPiC-FM.npy\",\"wb\") as f:\n",
    "    np.save(f,ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bravobenno.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(ast\u001b[38;5;241m.\u001b[39mliteral_eval(l)) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lists]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(arrays)\n\u001b[0;32m---> 13\u001b[0m results\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbravobenno.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_mean\u001b[39m(x):\n\u001b[1;32m     15\u001b[0m         x,w\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(x[\u001b[38;5;241m0\u001b[39m]),np\u001b[38;5;241m.\u001b[39marray(x[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/gpfs/dust/maxwell/user/kaechben/.conda/envs/mdma-cfm/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bravobenno.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from numpy import array\n",
    "import re\n",
    "# Convert string representation of lists back to lists\n",
    "def convert_str_to_tuple(s):\n",
    "    # Extract lists from the string using regex\n",
    "    s=s.replace(\"\\n \",\"\")\n",
    "    lists = re.findall(r'\\[.*?\\]', s)\n",
    "    arrays = [np.array(ast.literal_eval(l)) for l in lists]\n",
    "    return tuple(arrays)\n",
    "results=pd.read_csv(\"bravobenno.csv\")\n",
    "def weighted_mean(x):\n",
    "        x,w=np.array(x[0]),np.array(x[1])\n",
    "        weights=1/w**2\n",
    "        return np.sum(x*weights)/np.sum(weights)\n",
    "def weighted_std(x):\n",
    "    w=(1/np.array(x[1]))**2\n",
    "    sigma=np.sqrt(1/np.sum(w))\n",
    "    return sigma\n",
    "\n",
    "def format_mean_sd(mean, sd):\n",
    "    \"\"\"round mean and standard deviation to most significant digit of sd and apply latex formatting\"\"\"\n",
    "    decimals = -int(np.floor(np.log10(sd)))\n",
    "    decimals -= int((sd * 10 ** decimals) >= 9.5)\n",
    "\n",
    "    if decimals < 0:\n",
    "        ten_to = 10 ** (-decimals)\n",
    "        if mean > ten_to:\n",
    "            mean = ten_to * (mean // ten_to)\n",
    "        else:\n",
    "            mean_ten_to = 10 ** np.floor(np.log10(mean))\n",
    "            mean = mean_ten_to * (mean // mean_ten_to)\n",
    "        sd = ten_to * (sd // ten_to)\n",
    "        decimals = 0\n",
    "\n",
    "    if mean >= 1e3 and sd >= 1e3:\n",
    "        mean = np.round(mean * 1e-3)\n",
    "        sd = np.round(sd * 1e-3)\n",
    "        return f\"${mean:.{decimals}f}$k $\\\\pm {sd:.{decimals}f}$k\"\n",
    "    else:\n",
    "        return f\"${mean:.{decimals}f} \\\\pm {sd:.{decimals}f}$\"\n",
    "df=results\n",
    "df[\"pmm\"]=df[\"w1m\"].apply(ast.literal_eval).apply(lambda x:x).apply(weighted_std)\n",
    "df[\"w1m\"]=df[\"w1m\"].apply(ast.literal_eval).apply(lambda x:x).apply(weighted_mean)\n",
    "df[\"pmp\"]=df[\"w1p\"].apply(convert_str_to_tuple).apply(lambda x:x).apply(weighted_std)\n",
    "df[\"w1p\"]=df[\"w1p\"].apply(convert_str_to_tuple).apply(lambda x:x).apply(weighted_mean)\n",
    "df[\"pme\"]=df[\"w1efp\"].apply(convert_str_to_tuple).apply(lambda x:x).apply(weighted_std)\n",
    "df[\"w1efp\"]=df[\"w1efp\"].apply(convert_str_to_tuple).apply(lambda x:x).apply(weighted_mean)\n",
    "\n",
    "df[\"fpd_std\"]=df[\"fpd\"].apply(ast.literal_eval).apply(lambda x:x[1]).apply(np.mean)\n",
    "df[\"fpd\"]=df[\"fpd\"].apply(ast.literal_eval).apply(lambda x:x[0]).apply(np.mean)\n",
    "df[\"kpd_std\"]=df[\"kpd\"].apply(ast.literal_eval).apply(lambda x:x[1]).apply(np.mean)\n",
    "df[\"kpd\"]=df[\"kpd\"].apply(ast.literal_eval).apply(lambda x:x[0]).apply(np.mean)\n",
    "\n",
    "cols=[\"name\",\"w1m\",\"w1efp\",\"w1m\",\"pmm\",\"pme\",\"pmp\",\"cov\",\"mmd\",\"fpd\",\"kpd\",\"time\",\"parameters\"]\n",
    "replace_dict={\"MPGAN\":\"MPGAN\",\"t_cpflow\":\"PF\",\"t_ipflow\":\"IPF\",\"t_apflow\":\"APF\",\"t_nflow\":\"NF\",\"t_ccnflow\":\"NF(cc)\",\"t_cnflow\":\"NF(c)\",\"t_tnflow\":\"TNF\",\"IN\":\"IN\"}\n",
    "df.loc[:,\"model\"]=df[\"model\"].apply(lambda x:replace_dict[x])\n",
    "df=df.set_index(\"model\",drop=True)\n",
    "df.loc[:,\"w1m\"]*=1000\n",
    "df.loc[:,\"w1p\"]*=1000\n",
    "df[\"w1p\"]=df[\"w1p\"]\n",
    "df.loc[:,\"w1efp\"]*=100000\n",
    "df[\"w1efp\"]=df[\"w1efp\"]\n",
    "df.loc[:,\"pmm\"]*=1000\n",
    "df.loc[:,\"pmp\"]*=1000\n",
    "df.loc[:,\"pme\"]*=100000\n",
    "df.loc[:,\"fpd\"]*=10000\n",
    "df.loc[:,\"fpd_std\"]*=10000\n",
    "df.loc[:,\"kpd\"]*=10000\n",
    "\n",
    "df.loc[:,\"kpd_std\"]*=10000\n",
    "df.loc[:,\"w1m\"]=df.apply(lambda x:format_mean_sd(float(x[\"w1m\"]),float(x[\"pmm\"])),axis=1)\n",
    "df.loc[:,\"w1p\"]=df.apply(lambda x:format_mean_sd(float(x[\"w1p\"]),float(x[\"pmp\"])),axis=1)\n",
    "df.loc[:,\"w1efp\"]=df.apply(lambda x:format_mean_sd(float(x[\"w1efp\"]),float(x[\"pme\"])),axis=1)\n",
    "df.loc[:,\"kpd\"]=df.apply(lambda x:format_mean_sd(float(x[\"kpd\"]),float(x[\"kpd_std\"])),axis=1)\n",
    "df.loc[:,\"fpd\"]=df.apply(lambda x:format_mean_sd(float(x[\"fpd\"]),float(x[\"fpd_std\"])),axis=1)\n",
    "df.loc[:,\"time\"]*=1e6\n",
    "df.loc[:,\"time\"]=np.round(df[\"time\"],decimals=1)\n",
    "order=[\"PF\",\"IPF\",\"APF\",\"NF\",\"NF(c)\",\"NF(cc)\",\"TNF\",\"MPGAN\",\"IN\"]\n",
    "df=df.loc[order,:]\n",
    "print(df)\n",
    "# def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print (count_parameters(model.gen_net))\n",
    "tex=\"\"\n",
    "for p in [\"t\"]:\n",
    "    temp=df\n",
    "    for col in df.columns:\n",
    "        if col not in [\"w1m\",\"w1p\",\"w1efp\",\"fpd\",\"kpd\",\"time\"]:\n",
    "            continue\n",
    "        temp_index=temp[col].astype(str).str.replace(\"$\",\"\").str.replace(\"k\",\"\").str.split(\"\\\\\").str[0].astype(float)\n",
    "        mins=temp_index==temp_index.drop(\"IN\").min()\n",
    "        temp.loc[mins,col]=\"$\\mathbf{\"+temp.loc[mins,col].astype(str).str.replace(\"$\",\"\")+\"}$\"\n",
    "    temp=temp.reset_index()[[\"model\",\"w1m\",\"w1p\",\"w1efp\",\"kpd\",\"fpd\",\"time\"]]\n",
    "    temp.columns=[\"model\",\"$W_1^M (\\times 10^{3})$\",\"$W_1^P (\\times 10^{3})$\",\"$W_1^{EFP}(\\times 10^{5})$\",\"$KPD(\\times 10^{4})$\",\"$FPD$\",\"Time $\\mu s$\"]\n",
    "    text=temp.to_latex(index=False,escape=False)\n",
    "    parton=\"Gluon\" if p==\"g\" else \"Light Quark\" if p==\"q\" else \"Top Quark\"\n",
    "    tex+=\"\\multirow{9}{*}{\"+parton+\"} & \"+text.split(\"FPD\")[1].split(\"\\\\bottomrule\")[0].replace(\"\\\\\\\\\",\"\\\\\\\\&\").replace(\"\\\\midrule\",\"\").replace(\"  \",\"\")[:-2]+\"\\cline{1-8}\"\n",
    "    tex+=\"\\n\"\n",
    "print(tex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(array([0.00025178, 0.00022063, 0.00026026, 0.00043311, 0.00019063]), array([6.04386555e-07, 1.94986961e-06, 2.46654965e-06, 1.54915805e-06,\\n       1.93143100e-06]))'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"w1efp\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
